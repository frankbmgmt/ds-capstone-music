---
title: "Class Work Together Example"
author: "Francesco B."
date: "8/12/2019"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

This is a walk-thru led by Scott

```{r}
install.packages ('syuzhet')
install.packages ('ranger')
install.packages ('caret')
install.packages ('nnet')
#install.packages ('garson') Not abail for R version 3.6
```

```{r}
library ('tidyverse')
library ('syuzhet')
library ('tidytext')
library ('ranger')
library ('caret')
library ('nnet')
#library ('garson')
set.seed(123)
```

## Load Data
```{r}
raw_song <- read_csv("https://foco-ds-portal-files.s3.amazonaws.com/songdata.csv")
raw_genre = read_csv("https://foco-ds-portal-files.s3.amazonaws.com/Artists_Genre_Mapping.csv")
raw_dat = raw_song %>% 
  left_join(raw_genre, by = c('artist' = 'Band')) %>%
rename(genre = `Genre Updated`) %>%
  filter(!is.na(genre))%>%
  mutate(id = row_number())
# Downsize sample size
raw_dat = raw_dat %>%
  sample_n(10000)
```


## Wrangle Data
```{r}
dat = raw_dat %>%
  mutate(sentiment = get_sentiment(text),
          
         wrd_hottie = str_detect(text, "hottie"),
         wrd_gucci= str_detect(text, "gucci"),
         wrd_nigga= str_detect(text, "nigga"),
         wrd_bitch= str_detect(text, "bitch"),
         wrd_fuck= str_detect(text, "fuck")) %>%
  select(id, genre, sentiment, wrd_hottie, wrd_gucci)
dat
```

## EDA
```{r}
dat %>%
  ggplot(aes(x = genre, y = sentiment)) + geom_col() +
  coord_flip()
```
## Convert plot to mean sentiment like before!
```{r}
dat %>%
  filter(sentiment != 0) %>%
   group_by(genre) %>%
  summarize(mean_sentiment = mean(sentiment)) %>%
  ggplot(aes(x = genre, y = mean_sentiment)) + 
  geom_col() + 
  coord_flip()
```
# Most genres have a positive sentiment.


The most negative sentiment genres are:
- Rap, Hip-Hop and Metal.

The most positive sentiment genreas are:
- Religous
- Disco
- Easy-Listening
- Rock&Roll


## Do correlation plots of ones that look similar

```{r}
head(raw_dat)
```

## Deep Text Wrangling
```{r}
text_dat = raw_dat %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = 'word') %>%
group_by(genre, id, word) %>%
  summarize(n = 1) %>%
group_by(genre, word) %>%
  summarize(n = n()) %>%
  arrange(-n) %>%
  top_n(10)

head(text_dat)
```

# Look at at distributions

```{r}
text_dat %>%
  
  ggplot(aes(x = n)) +
  geom_density() +
    facet_wrap(~genre)
```

## This shows the most used words are mostly in near n=0


## Join dat & text_dat
```{r}
#TO DO... wait for Scott's code!!!!
final_dat = dat %>%
select(-id)
```

## Build Model

```{r}
training_split = 0.75
smp_size = floor(training_split * nrow(final_dat))
dat_index = sample(seq_len(nrow(final_dat)), size = smp_size)
dat_train = as.data.frame(final_dat[dat_index,])
dat_test = as.data.frame(final_dat[-dat_index,])

```

```{r}
train_control = trainControl(method = "oob")

mod_rf = train(dat_train %>% select(-genre),
            dat_train$genre,
            method = "ranger",
            num.trees = 56,
            importance = "impurity",
            
            trControl = train_control)

predictions_rf = predict(mod_rf, dat_test)
confusionMatrix(predictions_rf, as.factor(dat_test$genre))
```

```{r}
mod_rf$finalModel %>%
  # extract variable importance metrics
  ranger::importance() %>%
  # convert to a data frame
  enframe(name = "variable", value = "varimp") %>%
  top_n(n = 20, wt = varimp) %>%
  # plot the metrics
  ggplot(aes(x = fct_reorder(variable, varimp), y = varimp)) +
  geom_col() +
  coord_flip() +
  labs(x = "Token",
       y = "Variable importance (higher is more important)")


```

# Using Neural Network Model
```{r}
library(MASS)
```

```{r}

nnetModel <- train(dat_train, dat_train$genre,
                 method = "nnet",
                 preProcess = "range",
                 tuneLength = 2,
                 trace = FALSE,
                 maxit = 100)
predictions_rf = predict(nnetModel, dat_test)
confusionMatrix(predictions_rf, as.factor(dat_test$genre))
```
```{r}
#create a pretty color vector for the bar plot
#cols<-colorRampPalette(c('lightgreen','lightblue'))(num.vars)
 
#use the function on the model created above
#par(mar=c(3,4,1,1),family='serif')
#gar.fun('y',mod1,col=cols,ylab='Rel. importance',ylim=c(-1,1))
garson(nnetModel,)
```


#```{r}
nnetModel$finalModel %>%
  # extract variable importance metrics
  nnet::importance() %>%
  # convert to a data frame
  enframe(name = "variable", value = "varimp") %>%
  top_n(n = 20, wt = varimp) %>%
  # plot the metrics
  ggplot(aes(x = fct_reorder(variable, varimp), y = varimp)) +
  geom_col() +
  coord_flip() +
  labs(x = "Token",
       y = "Variable importance (higher is more important)")


#```